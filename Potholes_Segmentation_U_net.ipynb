{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Potholes_Segmentation_U_net.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VIVEKPATIL12/secondWeb/blob/main/Potholes_Segmentation_U_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the dataset repository\n",
        "!git clone https://github.com/VIVEKPATIL12/DataSet_DS.git\n",
        "\n",
        "# Step 1: Unzip both datasets into a combined directory\n",
        "!unzip -qq /content/DataSet_DS/DS1.zip -d /content/combined_folder\n",
        "!unzip -qq /content/DataSet_DS/DS2.zip -d /content/combined_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqufdaC4i1Fx",
        "outputId": "43f96fd9-a086-4006-9857-bc66a998221e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DataSet_DS'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 11 (delta 0), reused 8 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (11/11), done.\n",
            "Filtering content: 100% (2/2), 111.67 MiB | 29.87 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "classes = ['dry', 'snowy', 'wet']  # replace with your actual class names\n",
        "data_dir = '/home/vivekp22/Desktop/IVMC_SLIP_MU_ESTIMATION/Dataset/DS'\n",
        "train_dir = 'Dataset/train/'\n",
        "test_dir = 'Dataset/test/'\n",
        "\n",
        "# Create train and test directories\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "for cls in classes:\n",
        "    # Create class-specific train and test directories\n",
        "    os.makedirs(os.path.join(train_dir, cls), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, cls), exist_ok=True)\n",
        "\n",
        "    # Get a list of all image files for this class\n",
        "    image_files = os.listdir(os.path.join(data_dir, cls))\n",
        "\n",
        "    # Split the files into train and test sets\n",
        "    train_files, test_files = train_test_split(image_files, test_size=0.2, random_state=42)  # 80% train, 20% test\n",
        "\n",
        "    # Copy the train files into the train directory\n",
        "    for file in train_files:\n",
        "        shutil.copy(os.path.join(data_dir, cls, file), os.path.join(train_dir, cls, file))\n",
        "\n",
        "    # Copy the test files into the test directory\n",
        "    for file in test_files:\n",
        "        shutil.copy(os.path.join(data_dir, cls, file), os.path.join(test_dir, cls, file))\n",
        "\n",
        "# Train the Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import scipy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "# Define the data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'Dataset/train/',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    'Dataset/test/',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical')\n",
        "\n",
        "\n",
        "# Load the VGG16 network, ensuring the head FC layer sets are left off\n",
        "baseModel = VGG16(weights=\"imagenet\", include_top=False, input_tensor=tf.keras.Input(shape=(224, 224, 3)))\n",
        "\n",
        "# Construct the head of the model that will be placed on top of the base model\n",
        "headModel = baseModel.output\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(512, activation=\"relu\")(headModel)\n",
        "headModel = Dense(3, activation=\"softmax\")(headModel)  # Change the number here according to your number of classes\n",
        "\n",
        "# Place the head FC model on top of the base model (this will become the actual model we will train)\n",
        "model = tf.keras.Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# Loop over all layers in the base model and freeze them so they will not be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['accuracy'])\n",
        "\n",
        "# # Train the model\n",
        "# model.fit(train_generator, validation_data=test_generator, epochs=2)\n",
        "\n",
        "# # Save the model\n",
        "# model.save('model.h5')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bU1h1f9pi2oe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}